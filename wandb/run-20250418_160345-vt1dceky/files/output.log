/home/wxia/TransformerEM-GMM/exp/asset/em_pytorch.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X = torch.tensor(X)
/home/wxia/TransformerEM-GMM/exp/asset/em_pytorch.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  responsibilities[small_values_indices] = responsibilities[small_values_indices] + torch.tensor(random_additions)
/home/wxia/TransformerEM-GMM/exp/asset/em_pytorch.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  X = torch.tensor(X)
Traceback (most recent call last):
  File "/home/wxia/TransformerEM-GMM/exp/exp_second_round/usa_load_15minutes/train.py", line 87, in <module>
    _loss, _random_num, _new_para, _param, r_samples, r_samples_part, _mm = gmm_train_tool.get_loss_le(dataset, encoder,
  File "/home/wxia/TransformerEM-GMM/exp/asset/gmm_train_tool.py", line 76, in get_loss_le
    encoder_out = encoder(_train_sample_part_emb)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/TransformerEM-GMM/exp/model/gmm_transformer.py", line 143, in forward
    tokens = block(tokens)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/TransformerEM-GMM/exp/model/gmm_transformer.py", line 101, in forward
    x = x + self.mlp(self.norm2(x))
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/wxia/TransformerEM-GMM/exp/exp_second_round/usa_load_15minutes/train.py", line 87, in <module>
    _loss, _random_num, _new_para, _param, r_samples, r_samples_part, _mm = gmm_train_tool.get_loss_le(dataset, encoder,
  File "/home/wxia/TransformerEM-GMM/exp/asset/gmm_train_tool.py", line 76, in get_loss_le
    encoder_out = encoder(_train_sample_part_emb)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/TransformerEM-GMM/exp/model/gmm_transformer.py", line 143, in forward
    tokens = block(tokens)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/TransformerEM-GMM/exp/model/gmm_transformer.py", line 101, in forward
    x = x + self.mlp(self.norm2(x))
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxia/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
